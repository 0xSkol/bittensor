{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ae9311fd454b2f953566e36c419755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bittensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "# bittensor.logging( debug = True )\n",
    "dataset = bittensor.dataset( dataset_name = ['Books3'] )\n",
    "subtensor = bittensor.subtensor(  )\n",
    "graph = bittensor.metagraph( subtensor = subtensor ).sync()\n",
    "wallet = bittensor.wallet(name = 'default2', hotkey = 'default')\n",
    "dendrite = bittensor.dendrite (wallet = wallet)\n",
    "synapse = bittensor.TextCausalLM()\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def cal_loss(inputs, query_response, validation_len = 1):\n",
    "    _labels = inputs[:, 1:].contiguous()\n",
    "    _logits = query_response[:, :-1, :].contiguous()\n",
    "    loss = loss_fct(_logits.view(-1, _logits.size(-1)), _labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_uids = torch.randperm(graph.n)\n",
    "random_endpoints = [graph.endpoints[uid] for uid in perm_uids[:100]]\n",
    "inputs = next(dataset)\n",
    "responses, return_ops, times = dendrite.text(\n",
    "    endpoints=random_endpoints,\n",
    "    inputs=inputs,\n",
    "    synapses=[synapse],\n",
    "    timeout=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4665, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([cal_loss(inputs, r[0]) for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.tensor([cal_loss(inputs, r[0]).item() for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(31), tensor(3.4665)), (tensor(98), tensor(3.4901)), (tensor(26), tensor(3.5527)), (tensor(25), tensor(3.5834)), (tensor(53), tensor(3.6258)), (tensor(8), tensor(3.6329)), (tensor(47), tensor(3.6936)), (tensor(79), tensor(3.6974)), (tensor(44), tensor(3.7064)), (tensor(84), tensor(3.7064))]\n",
      "2 tensor(3.4315, grad_fn=<NllLossBackward0>)\n",
      "4 tensor(3.3560, grad_fn=<NllLossBackward0>)\n",
      "6 tensor(3.3608, grad_fn=<NllLossBackward0>)\n",
      "8 tensor(3.3829, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses_sorted = losses.sort()\n",
    "\n",
    "print(list(zip(losses_sorted[1][:10], losses_sorted[0][:10])))\n",
    "for i in range(2, 10, 2):\n",
    "    mixed_responses = sum([responses[idx][0]/i for idx in losses_sorted[1][:i]])\n",
    "    print(i, cal_loss(inputs, mixed_responses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1190605/411828831.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  l = kl_loss(F.log_softmax(responses[31][0].view(-1, 50258)), F.softmax(responses[idx][0].view(-1, 50258)))\n",
      "/tmp/ipykernel_1190605/411828831.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  l = kl_loss(F.log_softmax(responses[31][0].view(-1, 50258)), F.softmax(responses[idx][0].view(-1, 50258)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "1 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "2 tensor(288.5288, grad_fn=<KlDivBackward0>)\n",
      "3 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "4 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "5 tensor(227.5515, grad_fn=<KlDivBackward0>)\n",
      "6 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "7 tensor(227.5936, grad_fn=<KlDivBackward0>)\n",
      "8 tensor(118.9006, grad_fn=<KlDivBackward0>)\n",
      "9 tensor(229.4922, grad_fn=<KlDivBackward0>)\n",
      "10 tensor(117.5225, grad_fn=<KlDivBackward0>)\n",
      "11 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "12 tensor(288.4880, grad_fn=<KlDivBackward0>)\n",
      "13 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "14 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "15 tensor(131.3384, grad_fn=<KlDivBackward0>)\n",
      "16 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "17 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "18 tensor(148.3501, grad_fn=<KlDivBackward0>)\n",
      "19 tensor(121.3004, grad_fn=<KlDivBackward0>)\n",
      "20 tensor(227.8950, grad_fn=<KlDivBackward0>)\n",
      "21 tensor(131.3384, grad_fn=<KlDivBackward0>)\n",
      "22 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "23 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "24 tensor(227.5936, grad_fn=<KlDivBackward0>)\n",
      "25 tensor(80.7745, grad_fn=<KlDivBackward0>)\n",
      "26 tensor(117.2761, grad_fn=<KlDivBackward0>)\n",
      "27 tensor(121.3004, grad_fn=<KlDivBackward0>)\n",
      "28 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "29 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "30 tensor(288.5616, grad_fn=<KlDivBackward0>)\n",
      "31 tensor(1.1566e-06, grad_fn=<KlDivBackward0>)\n",
      "32 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "33 tensor(288.4880, grad_fn=<KlDivBackward0>)\n",
      "34 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "35 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "36 tensor(227.5327, grad_fn=<KlDivBackward0>)\n",
      "37 tensor(229.3431, grad_fn=<KlDivBackward0>)\n",
      "38 tensor(131.3384, grad_fn=<KlDivBackward0>)\n",
      "39 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "40 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "41 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "42 tensor(136.5880, grad_fn=<KlDivBackward0>)\n",
      "43 tensor(227.5936, grad_fn=<KlDivBackward0>)\n",
      "44 tensor(79.9200, grad_fn=<KlDivBackward0>)\n",
      "45 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "46 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "47 tensor(93.0306, grad_fn=<KlDivBackward0>)\n",
      "48 tensor(121.3004, grad_fn=<KlDivBackward0>)\n",
      "49 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "50 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "51 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "52 tensor(229.4554, grad_fn=<KlDivBackward0>)\n",
      "53 tensor(132.5529, grad_fn=<KlDivBackward0>)\n",
      "54 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "55 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "56 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "57 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "58 tensor(227.5515, grad_fn=<KlDivBackward0>)\n",
      "59 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "60 tensor(117.5095, grad_fn=<KlDivBackward0>)\n",
      "61 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "62 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "63 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "64 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "65 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "66 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "67 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "68 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "69 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "70 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "71 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "72 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "73 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "74 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "75 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "76 tensor(227.6326, grad_fn=<KlDivBackward0>)\n",
      "77 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "78 tensor(288.4028, grad_fn=<KlDivBackward0>)\n",
      "79 tensor(56.6479, grad_fn=<KlDivBackward0>)\n",
      "80 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "81 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "82 tensor(227.5936, grad_fn=<KlDivBackward0>)\n",
      "83 tensor(229.6286, grad_fn=<KlDivBackward0>)\n",
      "84 tensor(79.9200, grad_fn=<KlDivBackward0>)\n",
      "85 tensor(173.7447, grad_fn=<KlDivBackward0>)\n",
      "86 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "87 tensor(224.6296, grad_fn=<KlDivBackward0>)\n",
      "88 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "89 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "90 tensor(316.5561, grad_fn=<KlDivBackward0>)\n",
      "91 tensor(227.5936, grad_fn=<KlDivBackward0>)\n",
      "92 tensor(965.2291, grad_fn=<KlDivBackward0>)\n",
      "93 tensor(136.5880, grad_fn=<KlDivBackward0>)\n",
      "94 tensor(122.1703, grad_fn=<KlDivBackward0>)\n",
      "95 tensor(227.5915, grad_fn=<KlDivBackward0>)\n",
      "96 tensor(227.3867, grad_fn=<KlDivBackward0>)\n",
      "97 tensor(229.5237, grad_fn=<KlDivBackward0>)\n",
      "98 tensor(40.1837, grad_fn=<KlDivBackward0>)\n",
      "99 tensor(227.3867, grad_fn=<KlDivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "kl_loss = torch.nn.KLDivLoss(reduction=\"sum\")\n",
    "kl_losses = []\n",
    "for idx in range(100):\n",
    "    l = kl_loss(F.log_softmax(responses[31][0].view(-1, 50258)), F.softmax(responses[idx][0].view(-1, 50258)))\n",
    "    # print(idx, l)\n",
    "    kl_losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(3.3770, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(kl_losses, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50258])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# responses[31][0].shape\n",
    "responses[31][0].view(-1, 50258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([3, 5]) tensor(0.6772, grad_fn=<DivBackward0>)\n",
      "torch.Size([3, 5]) torch.Size([3, 5]) tensor(0.6024, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "# input should be a distribution in the log space\n",
    "input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "# Sample a batch of distributions. Usually this would come from the dataset\n",
    "target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "output = kl_loss(input, target)\n",
    "\n",
    "print(input.shape, target.shape, output)\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "log_target = F.log_softmax(torch.rand(3, 5), dim=1)\n",
    "output = kl_loss(input, log_target)\n",
    "\n",
    "print(input.shape, target.shape, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0227, 0.1587, 0.3786, 0.0382, 0.4018],\n",
       "        [0.0132, 0.1037, 0.1681, 0.0913, 0.6237],\n",
       "        [0.0084, 0.1362, 0.0405, 0.6494, 0.1656]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
