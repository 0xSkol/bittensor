diff --git a/bittensor/_wallet/wallet_impl.py b/bittensor/_wallet/wallet_impl.py
index 9fbabc7..a8866af 100644
--- a/bittensor/_wallet/wallet_impl.py
+++ b/bittensor/_wallet/wallet_impl.py
@@ -131,7 +131,23 @@ class Wallet():
             print (e)
             return bittensor.Balance(0)
 
-    def add_stake( self, 
+    def subscribe ( 
+        self,
+        axon: 'bittensor.Axon',
+        subtensor: 'bittensor.Subtensor' = None,
+    ):
+        # ---- Subscribe to chain ----
+        subscribe_success = subtensor.subscribe(
+                wallet = self,
+                ip = axon.external_ip,
+                port = axon.external_port,
+                wait_for_finalization = True,
+        )
+        if not subscribe_success:
+            raise RuntimeError('Failed to subscribe neuron.')
+        
+
+    def add_stake ( self, 
         amount: Union[float, bittensor.Balance] = None, 
         wait_for_inclusion: bool = True,
         wait_for_finalization: bool = False,
@@ -165,7 +181,7 @@ class Wallet():
             subtensor = bittensor.subtensor()
         return subtensor.add_stake( wallet = self, amount = amount, wait_for_inclusion=wait_for_inclusion, wait_for_finalization=wait_for_finalization )
 
-    def remove_stake( self, 
+    def remove_stake ( self, 
         amount: Union[float, bittensor.Balance] = None, 
         wait_for_inclusion: bool = True,
         wait_for_finalization: bool = False,
diff --git a/miners/text/template_server.py b/miners/text/template_server.py
index de77da0..ccdf1ca 100644
--- a/miners/text/template_server.py
+++ b/miners/text/template_server.py
@@ -117,17 +117,16 @@ def main( config ):
             group = wallet.hotkey.ss58_address[:20],
             save_code = True
         ):
-        wandb.watch( model, log = 'all', log_freq = 10 )
 
         # --- Run Forever.
         while True:
             metagraph.sync().save()
             uid = metagraph.hotkeys.index( wallet.hotkey.ss58_address )
             wandb_data = {
-                'Stake': metagraph.S[ uid ].item(),
-                'Rank': metagraph.R[ uid ].item(),
-                'Incentive': metagraph.I[ uid ].item(),
-                'Axon QPS': axon.stats.qps.value
+                'stake': metagraph.S[ uid ].item(),
+                'rank': metagraph.R[ uid ].item(),
+                'incentive': metagraph.I[ uid ].item(),
+                'axon QPS': axon.stats.qps.value
             } 
             for uid_i, val in enumerate(metagraph.W[:,uid].tolist()):
                 wandb_data[ 'w_{},{}'.format(uid_i, uid) ] = val
diff --git a/miners/text/template_validator.py b/miners/text/template_validator.py
index 0f47925..cd74c43 100644
--- a/miners/text/template_validator.py
+++ b/miners/text/template_validator.py
@@ -27,28 +27,41 @@ import torch
 import time
 import wandb
 import datetime
-from qqdm import qqdm
-from transformers import BertModel, BertConfig
+import torch.nn.functional as F
+from qqdm import qqdm, format_str
+from torch.nn import TransformerEncoder, TransformerEncoderLayer
 
 def config ():
     parser = argparse.ArgumentParser()
+    parser.add_argument('--miner.topk', type=int, help='the number of peers queried during each remote forward call', default=20)
     parser.add_argument('--miner.learning_rate', type=float, help='Training initial learning rate.', default=1)
     parser.add_argument('--miner.momentum', type=float, help='optimizer momentum.', default=0.8)
-    parser.add_argument('--miner.clip_gradients', type=float, help='Implement gradient clipping to avoid exploding loss on smaller architectures.', default=1.0)
+    parser.add_argument('--miner.epoch_length', type=int, help='Iterations of training per epoch', default=100)
+    parser.add_argument('--miner.n_topk_chain_weights', type=int, help='Maximum number of weights to submit to chain', default=100 )
     parser.add_argument('--miner.device', type=str, help='miner default training device cpu/cuda', default=("cuda" if torch.cuda.is_available() else "cpu"))
+    parser.add_argument('--nucleus.topk', type=int, help='the number of peers queried during each remote forward call', default=20)
+    parser.add_argument('--nucleus.punishment', type=float, help='The punishment on the chain weights that do not respond ', default=0.001 )
+    parser.add_argument('--nucleus.nhid', type=int, help='the dimension of the feedforward network model in nn.TransformerEncoder', default=200)
+    parser.add_argument('--nucleus.nhead', type=int, help='the number of heads in the multiheadattention models', default=2)
+    parser.add_argument('--nucleus.nlayers', type=int, help='the number of nn.TransformerEncoderLayer in nn.TransformerEncoder', default=2)
+    parser.add_argument('--nucleus.dropout', type=float, help='the dropout value', default=0.2)
     bittensor.wallet.add_args( parser )
-    bittensor.axon.add_args( parser )
+    bittensor.dendrite.add_args( parser )
     bittensor.subtensor.add_args( parser )
     bittensor.logging.add_args( parser )
+    bittensor.dataloader.add_args( parser )
     return bittensor.config( parser )
 
 def main( config ):
+    print (config)
 
     # Init bittensor logging.
     bittensor.logging( config = config )
 
     # Load/Create our bittensor wallet.
     wallet = bittensor.wallet( config = config ).create()
+    if wallet.get_balance().rao > 0:
+        wallet.add_stake()
 
     # Connect to the chain.
     subtensor = bittensor.subtensor( config = config )
@@ -56,33 +69,68 @@ def main( config ):
     # Load/Sync/Save our metagraph.
     metagraph = bittensor.metagraph ( subtensor = subtensor ).load().sync().save()
 
-    # Instantiate the model we are going to serve on the network.
-    # Miner training device.
+    # Create Dendrite.
+    dendrite = bittensor.dendrite( config = config )
+
+    # Genesis dataset.
+    dataset = bittensor.dataloader ( config = config )
+
+    # Build Device.
     device = torch.device( device = config.miner.device)
-    model = BertModel( 
-        BertConfig (
-            vocab_size = bittensor.__vocab_size__,
-            hidden_size = bittensor.__network_dim__,
-            num_hidden_layers = 8,
-            num_attention_heads = 8,
-            intermediate_size = 3072,
-            hidden_act = "gelu",
-            hidden_dropout_prob = 0.1,
-            attention_probs_dropout_prob = 0.1,
-            max_position_embeddings = 512,
-            type_vocab_size = 2,
-            initializer_range = 0.02,
-            layer_norm_eps = 1e-12,
-            pad_token_id = 0,
-            gradient_checkpointing = False,
-            position_embedding_type = "absolute",
-            use_cache = True,
-        )
-    ).to( device )
+
+    # Instantiate validator model.
+    class Validator(torch.nn.Module):
+        def __init__(self, config ):
+            super(Validator, self).__init__()
+            self.layers = TransformerEncoderLayer( bittensor.__network_dim__, config.nucleus.nhead, config.nucleus.nhid, config.nucleus.dropout )
+            self.encoder = TransformerEncoder( self.layers, config.nucleus.nlayers )
+            self.decoder = torch.nn.Linear( bittensor.__network_dim__, bittensor.__vocab_size__ , bias=False)
+            self.loss_fct = torch.nn.CrossEntropyLoss()
+            self.chain_weights = torch.nn.Parameter(torch.ones( [ metagraph.n.item() ] , requires_grad=True))
+
+        def forward( self ):
+            remote_hidden = self.remote( inputs.to( device ) )
+            encoded_hidden = self.encoder( remote_hidden )
+            decoded_targets = self.decoder ( encoded_hidden )
+
+            # Compute loss.
+            shift_logits = decoded_targets.local_target[..., :-1, :].contiguous()
+            shift_labels = inputs[..., 1:].contiguous()     
+            loss = self.loss_fct( shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1) )
+            return loss, decoded_targets
+
+        def remote ( self, inputs ):
+            # ---- Topk Weights ---- (TODO: check if the gaussians are enough disrupt the chain weights)
+            real_topk = min( config.nucleus.topk, metagraph.n.item() ) 
+            noise = torch.normal( 0, torch.std( self.chain_weights ).item()+0.0000001, size=( self.chain_weights.size())).to( device )
+            topk_weights, topk_uids = torch.topk( self.chain_weights + noise, real_topk, dim=0 ) 
+
+            # ---- Query network ----
+            responses, return_ops = dendrite.forward_text ( 
+                endpoints = metagraph.endpoints[ topk_uids ], 
+                inputs = inputs
+            )
+
+            # ---- Join based on weights ----
+            joining_uids = torch.where(return_ops==0)[0]
+            joining_weights = F.softmax( topk_weights[(return_ops == 0)], dim = 0 )
+            output = torch.zeros( (inputs.shape[0], inputs.shape[1], bittensor.__network_dim__)).to( device )
+            for index, joining_weight in enumerate( joining_weights ): 
+                output += responses[joining_uids[index]].to( device ) * joining_weight
+
+            # ---- Punish peers with non-successful return ops ----
+            with torch.no_grad():
+                self.chain_weights[topk_uids[(return_ops != 0)]] -= config.nucleus.punishment
+                self.chain_weights[ self.chain_weights < -1 ] = -1 # lower bound for chain weights 
+
+            return output
+
+    # Create validator model.
+    validator = Validator( config = config)
 
     # Create our optimizer.
     optimizer = torch.optim.SGD(
-        [ {"params": model.parameters()} ],
+        [ {"params": validator.parameters()} ],
         lr = config.miner.learning_rate,
         momentum = config.miner.momentum,
     )
@@ -95,24 +143,53 @@ def main( config ):
             group = wallet.hotkey.ss58_address[:20],
             save_code = True
         ):
-        wandb.watch( model, log = 'all', log_freq = 10 )
+        wandb.watch( validator.chain_weights, log = 'all', log_freq = 10 )
 
         # --- Run Forever.
         while True:
+
+            # --- Run epoch.
+            batches = dataset.dataloader( config.miner.epoch_length )
+            progress_bar = qqdm(enumerate(batches), total=len(batches), desc=format_str('blue', f'Epoch Progress'))
+            for _, (inputs) in progress_bar:
+                loss, _ = validator( inputs )
+                loss.backward()
+                optimizer.step()
+                optimizer.zero_grad() 
+
+            # ---  Set mechanism weights.
+            real_topk = min( config.miner.n_topk_chain_weights, metagraph.n.item() ) 
+            topk_weights, topk_uids = torch.topk( validator.chain_weights, k = real_topk )
+            normalized_topk_weights = torch.nn.functional.normalize( topk_weights - torch.min( topk_weights ), p = 1, dim = 0)
+            did_set = subtensor.set_weights(
+                uids = topk_uids,
+                weights = normalized_topk_weights,
+                wait_for_inclusion = True,
+                wallet = wallet,
+            )    
+
+            # --- Sync + reshape.      
+            metagraph.sync().save()
+            chain_growth = metagraph.n.item() - torch.numel(chain_growth)
+            validator.chain_weights = torch.nn.Parameter(torch.cat( [validator.chain_weights, torch.ones([chain_growth], dtype=torch.float32, requires_grad=True)]))
+            optimizer = torch.optim.SGD(
+                [ {"params": validator.parameters()} ],
+                lr = config.miner.learning_rate,
+                momentum = config.miner.momentum,
+            )
+
+            # --- Log.
             metagraph.sync().save()
             uid = metagraph.hotkeys.index( wallet.hotkey.ss58_address )
             wand_data = {
                 'Stake': metagraph.S[ uid ].item(),
                 'Rank': metagraph.R[ uid ].item(),
                 'Incentive': metagraph.I[ uid ].item(),
-                'Axon QPS': axon.stats.qps.value
             } 
-            for uid_i, val in enumerate(metagraph.W[:,uid].tolist()):
-                wand_data[ 'w_\{{},{}\}'.format(uid_i, uid) ] = val
+            for uid_j, val in enumerate(metagraph.W[uid,:].tolist()):
+                wand_data[ 'w_\{{},{}\}'.format( uid, uid_j ) ] = val
             wandb.log( wand_data )
-            time.sleep( 30 * bittensor.__blocktime__ )
+            time.sleep( 10 * bittensor.__blocktime__ )
 
 if __name__ == "__main__":
-    conf = config()
-    print (conf)
-    main( conf )
\ No newline at end of file
+    main( config() )
\ No newline at end of file
